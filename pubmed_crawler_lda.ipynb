{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tools\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the search terms(keywords) \n",
    "keywords = ['genetics','machine learning','autism']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:13<00:00,  1.30s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:11<00:00,  1.16s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:11<00:00,  1.18s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:10<00:00,  1.06s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:11<00:00,  1.14s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:11<00:00,  1.13s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:11<00:00,  1.13s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:10<00:00,  1.09s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:10<00:00,  1.08s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:06<00:00,  1.10s/it]\n"
     ]
    }
   ],
   "source": [
    "# Get abstracts \n",
    "abs_list = tools.fetch_abstract(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lematize abstract\n",
    "lemmatized_text = tools.batch_lemma(abs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save result of lemmatization\n",
    "pickle.dump(lemmatized_text,open('lemmatized_text.pkl','wb'))\n",
    "lemmatized_text = pickle.load(open('lemmatized_text.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0, current stop words are: \n",
      "\n",
      "Iter 1, current stop words are: variant,gene,identify,disorder,use,genetic,method,study,asd,datum,disease\n",
      "\n",
      "Iter 2, current stop words are: risk,variant,gene,identify,machine,disorder,network,use,genetic,method,study,asd,datum,diagnosis,individual,learning,disease,base\n",
      "\n",
      "Iter 3, current stop words are: variant,gene,identify,machine,disorder,network,human,use,genetic,method,datum,clinical,feature,base,brain,risk,study,asd,diagnosis,individual,disease,learning\n",
      "\n",
      "Iter 4, current stop words are: variant,gene,identify,machine,disorder,network,human,use,genetic,method,datum,analysis,clinical,feature,base,brain,risk,functional,study,asd,diagnosis,spectrum,individual,disease,learning\n",
      "\n",
      "Topic 17 strong, sub, sub title, class sub, strong class, title, class, accuracy, ados,\n",
      "\n",
      "Topic 13 sub, schizophrenia, diagnostic, cluster, psychiatric, high, connectivity, deficit, result,\n",
      "\n",
      "Topic 12 biological, selection, scz, dataset, information, difference, classification, patient, score,\n",
      "\n",
      "Topic 3 group, model, age, female, diagnostic, cortical, adhd, ocd, thickness,\n",
      "\n",
      "Topic 4 mutation, include, variable, region, novo, algorithm, phenotype, select, site,\n",
      "\n",
      "Topic 16 model, fold, propose, variation, validation, include, implicate, performance, extract,\n",
      "\n",
      "Topic 18 level, selection, protein, effect, sequence, mutation, associate, predict, genomic,\n",
      "\n",
      "Topic 8 number, candidate, propose, associate, approach, learn, similarity, pron propose, genome,\n",
      "\n",
      "Topic 2 mutation, classifier, similarity, noncode, complex, predict, challenge, highly, specific,\n",
      "\n",
      "Topic 14 behavioral, snp, change, module, evaluate, effect, behavior, population, function,\n",
      "\n",
      "Topic 19 respectively, age, year, compare, associate, cognitive, range, deletion, model,\n",
      "\n",
      "Topic 15 neural, subject, mouse, group, correlate, unaffected, difference, accuracy, control,\n",
      "\n",
      "Topic 7 interaction, social, mouse, develop, expression, behavior, neural, new, database,\n",
      "\n",
      "Topic 0 control, variation, condition, noncode, signal, case, diagnostic, medical, case control,\n",
      "\n",
      "Topic 10 research, result, detect, lifelong, neuroimaging, project, support, structural, demonstrate,\n",
      "\n",
      "Topic 9 predict, alter, mutation, rna, patient, splice, expression, cause, autistic,\n",
      "\n",
      "Topic 11 tissue, mutation, protein, variation, genome, tool, strong, cause, associate,\n",
      "\n",
      "Topic 5 biomarker, specific, previous, knowledge, strong, associate, report, symptom, expression,\n",
      "\n",
      "Topic 1 rare, biomarker, patient, subject, strong, control, abnormality, medicine, variation,\n",
      "\n",
      "Topic 6 ability, distinguish, control, relate, regional, recognize, recently, currently, dataset,\n",
      "\n",
      "Topic 17: 0.144 \n",
      "Topic 13: 0.074 \n",
      "Topic 12: 0.069 \n",
      "Topic 3: 0.068 \n",
      "Topic 4: 0.066 \n",
      "Topic 16: 0.056 \n",
      "Topic 18: 0.056 \n",
      "Topic 8: 0.055 \n",
      "Topic 2: 0.055 \n",
      "Topic 14: 0.054 \n",
      "Topic 19: 0.048 \n",
      "Topic 15: 0.045 \n",
      "Topic 7: 0.044 \n",
      "Topic 0: 0.040 \n",
      "Topic 10: 0.032 \n",
      "Topic 9: 0.027 \n",
      "Topic 11: 0.023 \n",
      "Topic 5: 0.022 \n",
      "Topic 1: 0.021 \n",
      "Topic 6: 0.001 \n"
     ]
    }
   ],
   "source": [
    "#Because all abstracts are about pre-defined keywords, the topics extracted from them will have many overlapping words. Use these words as new stop words to find more meaningful topics.\n",
    "new_sw = []\n",
    "custom_sw = [] # Define your own stop words\n",
    "log = 'Iter {}, current stop words are: {}'\n",
    "iter_num = 5\n",
    "for i in range(5):\n",
    "    custom_sw = set(custom_sw).union(new_sw) # Combine previous stop words with new stop words found by model\n",
    "    print(log.format(i,','.join(custom_sw)) + '\\n')\n",
    "    lda,new_sw = tools.lda_pipeline(lemmatized_text, ngram = 2,suggest_sw = True,sw_list = custom_sw,print_model = (i==iter_num-1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
